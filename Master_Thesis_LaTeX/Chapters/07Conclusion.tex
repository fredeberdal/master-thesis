To address problems and inefficiencies with Peer Code Reviews in education, the objective of this thesis was to find out what selection techniques perform best to select crucial code to put in a review. This was done by a controlled experiment with a set of optimal files already known for each project. Eight files were selected by each technique for each of the three test projects and the control project. The results of the experiment will now be discussed. \\

The findings indicate that all selection techniques are efficient in identifying many of the files that are crucial in a repository. However, some techniques had higher accuracy in the selection of crucial files, while other techniques were more consistent in their selection. The \textbf{Keyword Selection} technique is unique, as this technique could vary greatly depending on what keywords are used to identify the files and therefore has a more focused selection than the rest. \\

\noindent The objective of the thesis reads as follows:
\begin{quote}
    \textit{Find the code selection techniques that are most effective in ensuring that the most critical code is selected for review in educational contexts, and investigate how these methods enhance the review process}
\end{quote}

To help answer this objective, this chapter will discuss how the selection techniques compare to each other, why they made the selections they did, and whether these selections were adequate. In addition, the accuracy, coverage, and consistency of the techniques will be discussed. The question of how the results reflect the research questions will also be discussed. \\



\section{Discussion}
%Hva som fungerer og hvorfor det fungerer. Vise at man skjÃ¸nner systemet.

When it comes to the selection techniques, all four performed adequately for their respective metrics. However, none of the techniques made perfect selections compared to the optimal selections, which will be discussed in Section \ref{Selections_compared}. \\


\subsection{Comparison of Selections} \label{Selections_compared}
\begin{quote}
    \textbf{RQ1:} How do code selection methods compare to each other?
\end{quote}

% most reliable depending on different metrics. accuracy, consistency, coverage and overall performance?\\
In order to evaluate the selection techniques, I will consider the measures; accuracy, coverage, and consistency, based on their results on the test projects. \\

\begin{table}[H]
  \centering
  \begin{tabularx}{\textwidth}{>{\hsize=0.5\hsize}X >{\hsize=1.5\hsize}X}
    \textbf{Measure} & \textbf{Description} \\ [1ex] \hline 
    
    \textbf{Accuracy} & Determined by the degree to which the selections align with the optimal selections. \\ [1ex]
    
    \textbf{Coverage} & Depends on how comprehensively the repository/project is covered. \\ [1ex] 
    
    \textbf{Consistency} & Measured by the extent to which the selections match with the other techniques' selections. \\ \hline
  \end{tabularx}
  \caption{Evaluation measures for the selection techniques}
  \label{tab:Evaluation_Measures}
\end{table}


\subsubsection{Accuracy}
% Which method selected the most optimal files average?\\
When comparing the accuracy of all selection techniques with the optimal selections in Section \ref{optimal_selections}, several notable observations can be made. Although each technique identified a significant number of optimal file selections, none of the selection techniques achieved perfect accuracy in their selections. However, some techniques had better accuracy than others. Excluding the control project, the number of matching selections between the selection techniques and the optimal selections are shown in Table \ref{tab:Accuracy}.

\begin{table}[H]
  \centering
  \begin{tabularx}{\textwidth}{>{\hsize=1.4\hsize}X >{\hsize=0.6\hsize}X >{\hsize=0.6\hsize}X >{\hsize=0.6\hsize}X}
    \textbf{Technique} & \textbf{Project A} & \textbf{Project B} & \textbf{Project C} \\ [1ex] \hline 
    
    \textbf{Size} & 5 files & 6 files & 6 files \\ [1ex]
    
    \textbf{Keyword} & 5 files & 5 files & 7 files \\ [1ex] 
    
    \textbf{Cyclomatic Complexity} & 6 files & 7 files & 7 files \\ [1ex]
    
    \textbf{Combination} & 6 files & 6 files & 7 files \\ \hline

  \end{tabularx}
  \caption{Selected files per technique matching the optimal selections}
  \label{tab:Accuracy}
\end{table}

The average accuracy is calculated by adding, for each technique, the number of matching selections to the optimal selections together and dividing it by the number of projects. The average accuracy is therefore as follows; 5.7 for Size Selection, 5.7 for Keyword Selection, 6.3 for Combination Selection, and 6.7 for Cyclomatic Complexity Selection. This suggests that Cyclomatic Complexity was the most accurate selection technique, which is intriguing. Combination Selection is a combination of multiple techniques, which would make it natural to imply that it is more accurate since it takes more metrics into consideration when finding crucial files. The Combination Selection technique incorporates Cyclomatic Complexity along with additional measures to determine the selections, which could imply that including size or Halstead measures in the calculations may have reduced the technique's accuracy. \\

While none of the techniques got a perfect selection, there were cases where only one file was not selected from the optimal selections by techniques. Specifically Cyclomatic Complexity Selection in Project B, and all techniques except Size Selection in Project C. It also regarded some techniques in the Control Project, but this is exempt inclusion here since the optimal selections were not identified, but taken directly from the Size Selections. This means that Size Selection was the only technique that did not even get seven out of eight optimal files selected in any of the cases. As the most crucial code segments do not necessarily align with the file with most lines of code, it makes sense that this technique would have some selections that would not be optimal. This is also the case when considering the coverage of the technique. 


\subsubsection{Coverage}
Size Selection limits its coverage on the repositories by selecting based solely on lines of code. Although this measure often selects the code with the most logic or functionality, it can identify \textit{false positives} or less relevant files if it selects files with a lot of code but of little importance. This was the case in Project B where Size Selection selected \textit{queries.ts}, which contains no logic or functionality except SQL queries. This selection theoretically made sense since it had 131 lines of code, but there was no relevant functionality or code to review in this case. Because of cases like this, it is smarter to combine Size Selection with other selection techniques to negate this side effect. \\

Like Size Selection, Keyword Selection also lacks coverage, since it only searches for files containing specific keywords and does not consider other files of importance, even though these could be just as crucial to review. This is important to keep in mind if using this technique in the PCR process. While Keyword Selection lacks coverage, it still made some selections of optimal files none of the other techniques did. In Project A and C, it made respectively two and one selections of optimal selection all the other selection methods overlooked in their top eight selections. So what Keyword Selections lacks in coverage, it makes up for in relevant, focused and pinpointed selections. In Project C, if Keyword Selection and either Cyclomatic Complexity Selection or Combination Selection had been applied together, it would probably have managed to select all eight optimal files, as these techniques separately covered the optimal selections. By leveraging the strengths of multiple techniques, a more comprehensive and effective selection process could been achieved. If these techniques are utilized together, they complement each other by covering different aspects of the codebase. Combining these methods would result in selecting more than eight files initially, but by cross-referencing the selections, the most critical eight files can be identified based on their combined importance and complexity. The synergistic effect of using both techniques together leads to better coverage and accuracy in identifying files that are critical for thorough review. \\


\subsubsection{Consistency}
To measure consistency, the various selection techniques were compared with each other to obtain another relative measure of effectiveness. Compared with each other, the various techniques had significantly more matching selections than compared with the optimal selections. Combination Selection was the most consistent method, while Keyword Selection stood out as the least consistent method. The number of matching selections between the selection techniques is shown in Table \ref{tab:Consistency}

\begin{table}[H]
  \centering
  \begin{tabularx}{\textwidth}{>{\hsize=1\hsize}X >{\hsize=0.5\hsize}X >{\hsize=0.5\hsize}X >{\hsize=0.5\hsize}X >{\hsize=0.5\hsize}X}
    \textbf{Technique} & \textbf{Project A} & \textbf{Project B} & \textbf{Project C}  & \textbf{Control}\\ [1ex] \hline 
    
    \textbf{Size} & 6 files & 7 files & 7 files & 7 files \\ [1ex]
    
    \textbf{Keyword} & 3 files & 6 files & 6 files & 4 files \\ [1ex] 
    
    \textbf{Cyclomatic} & 7 files & 7 files & 8 files & 6 files \\
    \textbf{Complexity} & & & & \\ [1ex]
    
    \textbf{Combination} & 8 files & 7 files & 8 files & 7 files \\ \hline

  \end{tabularx}
  \caption{Selected files per technique matching the other techniques' selections}
  \label{tab:Consistency}
\end{table}

Combination Selection had all eight of its files selected by at least one other technique for projects A and C, and seven of the eight files for projects B and control, giving it an average consistency of 7,5. This means that its selections were consistent and close to perfect in all the projects. With such a high consistency, it is a good indicator that this technique performs the best in this measure. \\

Size Selection and Cyclomatic Complexity Selection came close with respectively 6,7 and 7,0 in average consistency. Keyword Selection in general had the most files selected that were not selected by the other techniques, with an average of 4,7. This indicates that Keyword Selection did not perform as well as the others. However, it can be explained by the specificity of the technique, which can be both an advantage and a disadvantage. Since the coverage was worse and the keywords used as metrics were limited to very specific functionality or areas of focus, it did not look for the most important files in general. This works if the course educator has a project where specific aspects is the important area to cover in a review.
The keywords in this experiment were limited to three metrics, which worked relatively well in this case. Had there been, let us say, 7 keywords used as metrics, the selections of this technique would most probably be covering more files, but the accuracy of the crucial files would most probably decrease due to larger selection. \\

A combination of Keyword Selection in conjunction with other selection techniques can therefore ensure that both crucial files are selected, but also files containing the specific functionality that might be the focus of the course which may not always contain as much complexity or lines of code as the crucial files. \\



%TENK ENKELT. VIST HVORDAN IMPLEMENTASJON KAN BLI. testet ulike teknikker som undersÃ¸ker i praksis. Dra konklusjoner at de ulike teknikkene har ulike resultat og burde brukes med dette i bakhodet. 

\subsection{Educational Implementation} \label{Educational_Implementation}
\begin{quote}
    \textbf{RQ2:} How can code selection techniques be implemented in educational contexts?
\end{quote}

To address the challenges of the review process in educational contexts, a multifaceted approach is necessary. Balancing the workload and focusing on critical code files are strategies that can enhance the efficiency and effectiveness of Peer Code Reviews. Educators can develop a more practical and sustainable code review process by implementing code selection in the PCR process that will support student learning and development. \\

While all the selection techniques have some merit to them, as they all selected a significant amount of crucial files in each case, one should be aware of the shortcomings before implementing any of them in PCR. In order to get good coverage, accuracy and consistency in the selections, it is not recommended to only implement a single selection technique, as this might be lacking on several measures. My recommendation from this experiment is therefore to include at least two techniques. Combination Selection is a combination of Size, Cyclomatic Complexity and Halstead measures, but it still was not a perfect method. Applying Combination Selection with Keyword Selection on the other hand can be an improvement on coverage and consistency, and ensure a thorough selection. \\

The specific nature of a project is important to consider and might also dictate the best approach to selecting its code. If it is a project to enhance the students' knowledge on hooks and state management and navigation in web applications, Keyword Selection will likely be a relevant technique as this has a more narrow coverage and a higher focus on specific functionality, but it should be applied with another technique that has better accuracy. This is a reminder that there is not a one-size-fits-all approach to Peer Code Reviews, highlighting the importance of customizing PCR strategies to align with the specific needs of each project. \\


Reviewer fatigue is one of the most prominent challenges with code reviews and Peer Code Reviews. The files a reviewer reviews later in the process is not covered as thorough and receive less attention and comments. Since this was an experimental and controlled experiment, it was not tested on students in order to observe the effects of reviewer fatigue on the review process with selection techniques applied. However, any kind of prioritization on the files, other than the typical alphabetical ordering, that selects crucial files before less crucial files will undoubtedly positively affect the review quality since they get attention before reviewer fatigue kicks in. I am therefore inclined to declare that all the selection techniques tested in this experiment improves review quality of PCR because of less reviewer fatigue on crucial files compared to the standard tools like Gerrit and Github which uses alphabetical ordering. \\

Selection of specific files does not just combat the reviewer fatigue problem, it also ensures that the workload for students is bearable, and that they do not lose motivation and engagement during the review. By not reviewing an entire codebase, but as in this experiment just a select 8 files, the student engagement should not decrease as much, and the review quality should increase - or at least not diminish. It is difficult to say how selection techniques affect engagement without testing the selection techniques in practice and observing student learning and engagement. However, PCR with applied selection techniques is less time consuming because of fewer files to review while still focusing on the crucial and important files, which means that it is highly likely that engagement increases without negatively impacting learning or review quality. \\



\subsection{Selection Technique Implications}
\begin{quote}
    \textbf{RQ3:} What are the implications of different code selection techniques?
\end{quote}
% This question investigates how various code selection techniques affect the learning experience and outcomes. both the effectiveness of various code selection techniques and their broader educational implications. By measuring how well these techniques identify and prioritize critical code, this research will determine their overall utility and value in educational contexts.

%What patterns emerge in the different selection techniques?\\

The implications with the different selection techniques have been discussed and mentioned in various sections already but will be summarized here. Size Selection is a technique with great accuracy and consistency, but lacks on coverage and should not exclusively be utilized if used in a PCR. It is essential to balance this metric with another to avoid neglecting smaller and important files. Combining Size Selection with other techniques that consider complexity or keywords can address the limitations of relying solely on lines of code. \\

While Keyword Selection gives a focused and pinpointed approach to the selection process, it is limited by its narrow search. When searching solely for specific keywords, it is highly likely to overlook certain files that could be just as important to include but does not contain the keywords. Using Keyword Selection is therefore specifically recommended if the PCR focuses on particular areas or subjects that are essential to include in the review. Since Keyword Selection can have bias towards certain patterns, incorporating other selection techniques can balance out this limitation. \\

Cyclomatic Complexity Selection is generally a good technique and selects files with accuracy and consistency sufficient enough to be used alone. Implementing this technique in the selection process will improve its effectiveness and ensure consistency and standardization among the selections, as it is a quantitative measure. This technique can help students understand the importance of writing maintainable and understandable code. However, only focusing on areas with high complexity can lead to the oversight of important but less complex areas. \\


Combining different metrics allows for a more thorough and comprehensive review, ensuring various dimensions of complexity. Combination Selection marginally outperformed Cyclomatic Complexity Selection. That should have been expected from the beginning of the experiment, as it incorporates Size, Cyclomatic Complexity, and Halstead measures. Nevertheless, given that it only provided slightly better results, it could imply that there may be minimal unexplored differences between the two techniques, indicating that both might be close to an optimal selection approach. In educational contexts, using a combination technique can provide richer learning experiences as the students are exposed to different aspects of code complexity. As with Cyclomatic Complexity, using a combination of techniques can increase the objectivity of the review process since the metrics used are quantifiable. Using a Combination Selection, however, could present challenges if it requires the use of tools.



\section{Limitations}
There are several limitations that might have influenced the thesis to go in certain directions or introduced bias towards specific results. These limitations will be discussed in this section. \\

Since the optimal selections in Section \ref{optimal_selections} were identified by me and there was no other double-checking, there is a possibility that these are not actually the ideal selections. My expertise and experience are not perfect and can overlook certain aspects of the projects. \\

This experiment focused on testing projects written in TypeScript. Although JavaScript files and tests were included in the projects, the main focus was still TypeScript projects. Since the focus was TypeScript projects, the results might vary if the experiment is performed anew with projects written in other programming languages, since different languages have different features and complexities that could affect the results. \\

The Size, Cyclomatic Complexity, and Combination techniques have metrics that are covariant, which means they overlap in certain areas. More lines means more code, which increases the chance for more paths, or in other words, increases the Cyclomatic Complexity, and both of these are strongly related to Combination Selection.  This might have led these results to appear more consistent with each other than with Keyword Selection. \\

There was limited testing because there was no standardized template or experimental process to follow. Standardization could ensure greater consistency and reliability in the findings, reducing potential variability. If the testing had followed a standardized template and process, it might have impacted the proceedings and results. \\

The test projects were retrieved from only one project in a single course, IT2810, with the same project description. Selecting test projects from a variety of different courses and project descriptions could have produced different results. This uniformity might have led to biased results in this specific case, with the specific guidelines provided by the course professor. The control project could also have been selected using more standardized criteria, to make sure it had more in common with the test projects.\\

Lastly, the testing was performed exclusively within a programming environment (IDE), without involving any testing from students or developers. This limited testing might have narrowed down the results. A better approach would have included having a group of students perform Peer Code Reviews on projects both, without any selection technique and with the various selection techniques applied to the projects. Afterwards, the students would give feedback on the entire process. They would also answer a questionnaire covering different metrics such as perceived usefulness, perceived learning outcome, perceived review quality, and data about the comprehensiveness of bug detection and code quality feedback. This approach could have provided more comprehensive insight and a better understanding of how the selection of crucial code files in a review process impact review quality and the student motivation and learning benefits.


\section{Findings}
To find out which code selection methods are most effective in ensuring that the most critical code is reviewed, a literature review and an experiment were performed. The literature review gives an overview of how Peer Code Review is situated in today's educational and industrial era. PCR is an essential component in modern educational contexts, and focuses on knowledge sharing, learning outcomes and fostering team collaboration more than ever before. While there are still challenges with modern code reviews such as reviewer fatigue, tool limitations and reviewer expertise, its efficiency is ever-growing when the process gets more effective preparation and a more practical review process. \\

The chosen selection methods for the experiment were Size, Keyword, Cyclomatic Complexity and Combination Selection. All techniques have unique metrics and measures to compute which files are most crucial and should be included in a review. The techniques were applied to three projects from the IT2810 course with the same project description and one open source TypeScript project. To evaluate the techniques, the results were compared with a set of optimal selections for each project, as well as compared to each other. \\

The findings show that every technique had varied results, but they all selected a significant number of optimal files. Although none of the techniques had perfect accuracy, the average accuracy of eight files was 5.7 for Size Selection, 5.7 for Keyword Selection, 6.3 for Combination Selection, and 6.7 for Cyclomatic Complexity Selection. The Size Selection and Keyword Selection techniques lack coverage compared to the two other techniques, since their selection metrics could be skewed and give false positives. Combination Selection was the technique that had the best consistency when all the techniques were compared with each other. Across the four projects tested on, the Combination technique's selections were always chosen by at least one other technique, except for one file in project B and one in the Control project. \\

A combination of more techniques, or at least more than that used in the Combination Selection in this thesis, could have worked even better. This Combination technique only combined Size, Cyclomatic Complexity and Halstead measures, which still resulted in an imperfect selection. Keyword Selection showed great promise, but lacked coverage and consistency. If used in conjunction with other techniques, it could perform the best in an educational context where the learning outcome is defined and certain keywords are more relevant to review to reach the learning goal of the course. However, it remains unclear whether the arbitrarily amalgamation of techniques will yield better results. The integration of an effective technique with an ineffective one does not inherently guarantee better selection results. There are many considerations to make, and it is important to reflect on the optimal method of technique combination before proceeding with the process. Solely using Size Selection or Keyword Selection is also not recommended, since these techniques alone can come up short, but if combined with a technique like Cyclomatic Complexity or Halstead measures, it becomes more reliable. \\


%Summarize the strengths and weaknesses of each technique and suggest scenarios where one might be preferred over the others.


\section{Future work}
The findings of this thesis suggest that the inclusion of selection techniques in the Peer Code Review process improves the efficiency, engagement and lessens the workload for students. However, there are still areas that need to be explored further. Therefore, looking at the educational effect of selective review in PCR by performing experiments with student targets to observe the effect of selection techniques on motivation, engagement, review quality, and learning outcome can be interesting. This could also be done to observe the effect of reviewer fatigue and workload on projects with selection techniques applied compared to projects without them. \\

Another direction is to explore whether it is possible and even more efficient to select certain code segments from repository files rather than the whole file for review. Thus, increasing the focus of the review even further and eliminating review on unnecessary code segments. This will focus solely on the areas that are crucial, such as logic, functionality, and improved learning outcome. A possible negative impact this direction can have is that the holistic picture is dissolved even further and the learning outcome can become skewed because the reviewer does not see the bigger picture. \\

Defining a standardized experimental design to ensure future research follows the same template can be an important step to improve the research on the PCR topic. This would involve that certain conditions must be met when experimenting with selection techniques, involving student testing, etc. Having a standardized template to follow will ensure that different research can be compared and will promote better consistency in results and findings. \\

There are various selection techniques that can be explored in future research to expand the horizon of which methods and combinations of methods yield the best selection results. These techniques could be; change frequency, bug history, dependency and code smells. It is important to be aware that some of these techniques, such as change frequency and bug history, are less relevant in educational contexts. In these contexts, there often is no history to compare code to, since it often is just one delivery with the complete codebase. \\

As mentioned in Section \ref{Educational_Implementation}, applying Combination Selection with Keyword Selection could be an improvement on coverage and consistency, and ensure a thorough selection. How to properly combine these techniques and ensure they are weighed appropriately in conjunction is an interesting direction to research further, and could provide a more thorough selection technique.


%Include a section about what should or could be done in future research or explain any recommended next steps based on the results you got. This should be the last section in the discussion. \\

%Further Studies: Propose further empirical studies to verify these findings in a broader range of projects and environments. Research could specifically focus on developing and testing new metrics or combinations of metrics that might yield better coverage and accuracy.

%Experimental Validation: Suggest experimental setups where different selection techniques are applied in controlled environments to directly compare their effectiveness in identifying critical code issues.


